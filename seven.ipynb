{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Seven: Recurrent Network Architectures\n",
    "## Blake Gebhardt, Christian Gould, Caleb Moore\n",
    "dataset: https://www.kaggle.com/datasets/datatattle/email-classification-nlp\n",
    "\n",
    "In this lab, you will select a prediction task to perform on your dataset, evaluate a recurrent architecture and tune hyper-parameters. If any part of the assignment is not clear, ask the instructor to clarify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from numpy.linalg import pinv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook setup\n",
    "# from IPython.display import HTML\n",
    "# HTML('''<script>\n",
    "# code_show_err=false; \n",
    "# function code_toggle_err() {\n",
    "#  if (code_show_err){\n",
    "#  $('div.output_stderr').hide();\n",
    "#  } else {\n",
    "#  $('div.output_stderr').show();\n",
    "#  }\n",
    "#  code_show_err = !code_show_err\n",
    "# } \n",
    "# $( document ).ready(code_toggle_err);\n",
    "# </script>\n",
    "# To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation (3 points total)\n",
    "[2 points] Define and prepare your data set. Provide details about the source of the data. Discuss methods of tokenization in your dataset as well as any decisions to force a specific length of sequence.  Also discuss your rationale for the size and nature of your vocabulary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we selected for this project is an NLP dataset focused on classifying emails as spam or not spam. The primary features of the dataset consist of the email content, and the target variable is the binary label indicating whether the email is considered spam or not. \n",
    "\n",
    "In preparing the dataset, we will consider the possible necessity of first preprocess the raw email text by performing some or all of the following steps:\n",
    "\n",
    "1. Lowercasing: Convert all text to lowercase to ensure uniformity and reduce the vocabulary size.\n",
    "2. Remove HTML tags: Strip out any HTML tags present in the email content.\n",
    "3. Remove URLs, email addresses, and numbers: Replace these entities with special tokens (e.g., `<url>`, `<email>`, and `<number>`), which helps to generalize the model and reduce the vocabulary size.\n",
    "4. Remove special characters and punctuation: Simplify the text by removing any unnecessary symbols.\n",
    "5. Tokenization: Convert the preprocessed text into a sequence of tokens (words or subwords) that can be used to train the model.\n",
    "\n",
    "For tokenization, we have several options, including word-level, subword-level, and character-level tokenization. Word-level tokenization involves splitting the text into individual words, whereas subword-level tokenization divides the text into smaller units, such as morphemes or n-grams. Character-level tokenization, on the other hand, treats each character as a token. \n",
    "\n",
    "In this project, we will use subword-level tokenization, specifically Byte Pair Encoding (BPE), as it strikes a balance between word-level and character-level tokenization. BPE helps to capture both common words and rare words by splitting them into smaller, more frequent subwords. This approach reduces the risk of out-of-vocabulary issues and provides better generalization for unseen data.\n",
    "\n",
    "Regarding the sequence length, we will pad or truncate the tokenized sequences to a fixed length. This decision ensures that the input data has a consistent shape when fed into the neural network. The choice of sequence length should be based on an analysis of the email length distribution in the dataset, selecting a value that covers a significant portion of the emails without being too large or too small. For instance, if 95% of the emails are shorter than 500 tokens, we can set the sequence length to 500.\n",
    "\n",
    "The size and nature of the vocabulary will be determined during the BPE tokenization process. To strike a balance between computational efficiency and model performance, we can choose a vocabulary size based on the trade-off between these factors. A larger vocabulary size will better capture the nuances of the email content, but it may also lead to increased memory usage and longer training times. Conversely, a smaller vocabulary size will be more computationally efficient but might compromise the model's ability to understand rare words and phrases. Based on empirical evidence from similar NLP tasks, a vocabulary size of around 30,000 to 50,000 tokens is a reasonable starting point.\n",
    "\n",
    "In summary, the chosen dataset focuses on spam/notspam emails, and we will preprocess the email content through a series of steps, including tokenization using BPE. We will pad or truncate the tokenized sequences to a fixed length, and the size and nature of our vocabulary will be determined based on computational efficiency and model performance considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num                                       Message_body     Label\n",
      "0    1                         Rofl. Its true to its name  Non-Spam\n",
      "1    2  The guy did some bitching but I acted like i'd...  Non-Spam\n",
      "2    3  Pity, * was in mood for that. So...any other s...  Non-Spam\n",
      "3    4               Will � b going to esplanade fr home?  Non-Spam\n",
      "4    5  This is the 2nd time we have tried 2 contact u...      Spam\n"
     ]
    }
   ],
   "source": [
    "# getting data \n",
    "data = pd.read_csv(\"./data/SMS.csv\")\n",
    "\n",
    "# Tokenize the text and create sequences\n",
    "max_words = 10000  # Maximum number of words to keep\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(data['Message_body'])\n",
    "sequences = tokenizer.texts_to_sequences(data['Message_body'])\n",
    "\n",
    "# Determine input_length\n",
    "email_lengths = [len(seq) for seq in sequences]\n",
    "avg_length = np.mean(email_lengths)\n",
    "std_length = np.std(email_lengths)\n",
    "input_length = int(avg_length + std_length)\n",
    "\n",
    "# Pad or truncate sequences to input_length\n",
    "X = pad_sequences(sequences, maxlen=input_length)\n",
    "\n",
    "# Set input_dim and output_dim\n",
    "input_dim = max_words\n",
    "output_dim = 100\n",
    "\n",
    "X = data[\"Message_body\"].to_numpy()\n",
    "y = data[\"Label\"].to_numpy()\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "# can check these values here if need be\n",
    "# print(input_length)\n",
    "# print(input_dim)\n",
    "# print(output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenizer with a specified vocabulary size\n",
    "vocab_size = 10000\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "\n",
    "# Fit the tokenizer on the email texts\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# Convert the email texts to sequences of integers\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Pad the sequences to have the same length\n",
    "max_sequence_length = 100\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# Replace the original 'X' DataFrame with the preprocessed and tokenized data\n",
    "X = pd.DataFrame(X_padded)\n",
    "\n",
    "# init k fold stuff \n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "# change the y to 0 and 1\n",
    "# primitive encoding\n",
    "new_y = []\n",
    "for val in y:\n",
    "    if val == 'Non-Spam':\n",
    "        new_y.append(0)\n",
    "    else:\n",
    "        new_y.append(1)\n",
    "y = np.array(new_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.5 points] Choose and explain what metric(s) you will use to evaluate your algorithm’s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an email spam classification task using Recurrent Neural Network (RNN) architectures, the primary goal is to accurately distinguish between spam and non-spam emails while minimizing false classifications, especially false positives (i.e., legitimate emails marked as spam). This is because false positives can have a significant impact on user experience, leading to missed important emails, reduced trust in the email provider, and potential business losses. In light of this, accuracy alone may not be the best evaluation metric, as it does not provide a comprehensive view of the model's performance, particularly with respect to false positives and false negatives.\n",
    "\n",
    "Considering the importance of minimizing false positives and false negatives, we propose using the following metrics to evaluate the algorithm's performance:\n",
    "\n",
    "1. Precision: Precision, also known as positive predictive value, measures the proportion of true positive classifications (correctly identified spam emails) among all positive classifications (all emails predicted as spam). Precision is an essential metric for the task because it helps assess the reliability of the classifier in identifying spam emails. A high precision indicates that the model is effective at minimizing false positives.\n",
    "\n",
    "2. Recall: Recall, also known as sensitivity or true positive rate, measures the proportion of true positive classifications (correctly identified spam emails) among all actual positive instances (all real spam emails in the dataset). Recall is crucial for the task because it assesses the classifier's ability to identify all spam emails, ensuring that the majority of spam emails are detected. A high recall means the model is effective at minimizing false negatives.\n",
    "\n",
    "3. F1 Score: The F1 Score is the harmonic mean of precision and recall, providing a single metric that balances the trade-off between these two metrics. F1 Score is especially useful when dealing with imbalanced datasets, where the number of spam and non-spam emails may be significantly different. By optimizing for F1 Score, we can find a model that strikes a balance between minimizing both false positives and false negatives.\n",
    "\n",
    "4. Area Under the Receiver Operating Characteristic (ROC) Curve (AUC-ROC): The AUC-ROC measures the classifier's ability to distinguish between the two classes (spam and non-spam) across different classification thresholds. It represents the trade-off between the true positive rate (recall) and the false positive rate. A higher AUC-ROC indicates a better classifier performance. This metric is particularly helpful in evaluating the model's performance in the presence of varying class distributions or when the cost of false positives and false negatives differ.\n",
    "\n",
    "5. Confusion Matrix: The confusion matrix is a table that presents the number of true positives, false positives, true negatives, and false negatives for the classifier. It provides a more detailed view of the classifier's performance, allowing us to identify any specific issues or trends in the model's predictions, such as whether the model is biased towards a particular class.\n",
    "\n",
    "To conclude, using a combination of precision, recall, F1 Score, AUC-ROC, and the confusion matrix will provide a comprehensive evaluation of the algorithm's performance on the email spam classification task. These metrics allow us to assess the classifier's effectiveness in terms of both identifying spam emails and minimizing false positives and false negatives, ultimately ensuring a reliable and user-friendly email experience."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.5 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your train/test splitting method is a realistic mirroring of how an algorithm would be used in practice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dividing the email spam classification dataset into training and testing sets, we propose using Stratified K-Fold Cross-Validation, specifically with k=10 (Stratified 10-fold Cross-Validation). This method is chosen for several reasons, which are outlined below:\n",
    "\n",
    "1. Class Distribution Preservation: Stratified K-Fold Cross-Validation ensures that the proportion of spam and non-spam emails in each fold is approximately the same as the overall dataset. This is particularly important for imbalanced datasets, where one class may be significantly under-represented. By preserving the class distribution, we can better evaluate the model's performance on both classes and reduce the risk of overfitting on the majority class.\n",
    "\n",
    "2. Model Robustness: Stratified 10-fold Cross-Validation trains and tests the model on 10 different combinations of training and testing data, reducing the impact of any specific data split on the model's performance evaluation. This helps to provide a more reliable estimate of the model's true performance, as it considers various data distributions and potential biases.\n",
    "\n",
    "3. Bias and Variance Reduction: By averaging the performance metrics across the 10 folds, we can obtain a more stable and accurate performance estimate, reducing both bias (by considering multiple training sets) and variance (by averaging multiple test sets). This ensures that our model evaluation is more reliable and generalizable to unseen data.\n",
    "\n",
    "4. Realistic Evaluation: In practice, an email spam classification algorithm would be applied to a continuous stream of incoming emails, with a potentially changing distribution of spam and non-spam emails over time. Stratified 10-fold Cross-Validation, by considering multiple training and testing splits, provides a more realistic approximation of the algorithm's performance in such a dynamic environment. It ensures that our model is not overly reliant on any specific data split and can adapt to different distributions of emails.\n",
    "\n",
    "However, to further ensure the robustness of our model evaluation and to simulate real-world deployment, we can also consider using a combination of Stratified K-Fold Cross-Validation and Shuffle Splits. Shuffle Splits involve randomly partitioning the dataset into training and testing sets multiple times, providing an additional layer of randomness to the data splits. By combining both methods, we can assess the model's performance across a diverse range of data splits, further enhancing the reliability and generalizability of our performance evaluation.\n",
    "\n",
    "In summary, using Stratified 10-fold Cross-Validation and potentially combining it with Shuffle Splits provides a reliable and realistic method for dividing the email spam classification dataset into training and testing sets. This approach ensures that the model's performance evaluation considers multiple data distributions, preserves class distribution, and reduces the impact of specific data splits on the model's performance, ultimately resulting in a more robust and generalizable classifier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling (6 points total)\n",
    "[2 points] Investigate at least two different recurrent network architectures  Be sure to use an embedding layer . Adjust hyper-parameters of the networks as needed to improve generalization performance (train a total of at least four models). Discuss the performance of each network and compare them. Justify your choice of parameters for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dim = vocabulary size\n",
    "# output_dim = embedding dimension\n",
    "# input_length = length of input sequences\n",
    "\n",
    "# model 1\n",
    "def model1(input_dim, output_dim, input_length):\n",
    "\n",
    "    model = Sequential([\n",
    "    Embedding(input_dim, output_dim, input_length=input_length),\n",
    "    SimpleRNN(128, return_sequences=True),\n",
    "    SimpleRNN(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2\n",
    "def model2(input_dim, output_dim, input_length):\n",
    "\n",
    "    model = Sequential([\n",
    "    Embedding(input_dim, output_dim, input_length=input_length),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 3\n",
    "def model3(input_dim, output_dim, input_length):\n",
    "\n",
    "    model = Sequential([\n",
    "    Embedding(input_dim, output_dim, input_length=input_length),\n",
    "    GRU(128, return_sequences=True),\n",
    "    GRU(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, epochs=20):\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), verbose=1)\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p (10000, 100, 100)\n",
      "Fold Number:  1\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 4s 125ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.0000e+00 - accuracy: 0.8682 - val_loss: 0.0000e+00 - val_accuracy: 0.8768\n",
      "Fold Number:  2\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.0000e+00 - accuracy: 0.8267 - val_loss: 0.0000e+00 - val_accuracy: 0.8682\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0000e+00 - accuracy: 0.8768 - val_loss: 0.0000e+00 - val_accuracy: 0.8682\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0000e+00 - accuracy: 0.8768 - val_loss: 0.0000e+00 - val_accuracy: 0.8682\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0000e+00 - accuracy: 0.8768 - val_loss: 0.0000e+00 - val_accuracy: 0.8682\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0000e+00 - accuracy: 0.8768 - val_loss: 0.0000e+00 - val_accuracy: 0.8682\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0000e+00 - accuracy: 0.8768 - val_loss: 0.0000e+00 - val_accuracy: 0.8682\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0000e+00 - accuracy: 0.8768 - val_loss: 0.0000e+00 - val_accuracy: 0.8682\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0000e+00 - accuracy: 0.8768 - val_loss: 0.0000e+00 - val_accuracy: 0.8682\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0000e+00 - accuracy: 0.8768 - val_loss: 0.0000e+00 - val_accuracy: 0.8682\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0000e+00 - accuracy: 0.8768 - val_loss: 0.0000e+00 - val_accuracy: 0.8682\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# If this is true, it runs the slow version\n",
    "run_slow = False\n",
    "params = (input_dim, output_dim, 100)\n",
    "print('p', params)\n",
    "# Function to plot the training and validation accuracy and loss\n",
    "def plot_history(history, title):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Train and evaluate all model variations. Removed the first variation for now\n",
    "model_variations = [(model1, params), (model2, params)]\n",
    "# model_variations = [(model_A, 1), (model_A, 2), (model_B, 1), (model_B,2)]\n",
    "histories = []\n",
    "models = []\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    print(\"Fold Number: \", len(histories) + 1)\n",
    "    X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    sub_histories = []\n",
    "    sub_models = []\n",
    "    for model_func, variation in model_variations:\n",
    "        \n",
    "        model = model_func(variation[0], variation[1], variation[2])\n",
    "        history, model = train_and_evaluate_model(model, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, epochs=20 if run_slow else 10)\n",
    "        sub_histories.append(history)\n",
    "        sub_models.append(model)\n",
    "\n",
    "    histories.append(sub_histories)\n",
    "    models.append(sub_models)\n",
    "\n",
    "# Visualize the performance of all model variations\n",
    "for idx, (model_func, variation) in enumerate(model_variations):\n",
    "    title = f\"{model_func.__name__} Variation {variation}\"\n",
    "    for i in range(len(histories)):\n",
    "        plot_history(histories[i][idx], title=f\"{title} - Fold {i+1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1 point] Use the Glove pre-trained embedding. Provide justification of the embedding size."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1 points] Using the best parameters and architecture from the RNN in the previous step, add a second recurrent chain to your RNN. The input to the second chain should be the output sequence of the first chain. Visualize the performance of training and validation sets versus the training iterations. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.5 points] Use the method of train/test splitting and evaluation criteria that you argued for at the beginning of the lab."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.5 points] Run to convergence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1 point]  Visualize the results of all the RNNs you trained.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work (1 points total)\n",
    "You have free rein to provide additional analyses.\n",
    "One idea (required for 7000 level students to do one of these options):\n",
    "Research and use the ConceptNet, Numberbatch embedding and compare to GloVe. Which method is better for your specific application? \n",
    "Another Idea (NOT required): Try to create a RNN for generating novel text. \n",
    "nd GRU). Alternatively, you may also choose one recurrent network and one convolutional network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12e32e4eb567c064dfb2ee726c768b62ffee357f672fbe1693111e5d236a3b44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
